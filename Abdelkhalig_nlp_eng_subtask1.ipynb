{"cells":[{"cell_type":"code","source":["# --- INSTALL DEPENDENCIES ---\n","!pip install transformers datasets scikit-learn wandb accelerate sentencepiece -q"],"metadata":{"id":"FWzmynQWsNTM","executionInfo":{"status":"ok","timestamp":1765893621007,"user_tz":-120,"elapsed":8938,"user":{"displayName":"Abdelkhalig Elfatih (Khal)","userId":"06815457852751117892"}}},"execution_count":1,"outputs":[],"id":"FWzmynQWsNTM"},{"cell_type":"markdown","source":["# Trial1: deBERTa-v3-base"],"metadata":{"id":"dDyyF0YKIJXi"},"id":"dDyyF0YKIJXi"},{"cell_type":"code","source":["# Polarization Detection Shared Task - Subtask 1\n","# Binary Classification: Polarized vs Non-Polarized\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","from torch.utils.data import Dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    EarlyStoppingCallback,\n","    set_seed\n",")\n","from scipy.special import expit\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set seed for reproducibility\n","set_seed(42)\n"],"metadata":{"id":"y47ZhV2z3zbR","executionInfo":{"status":"ok","timestamp":1765893670981,"user_tz":-120,"elapsed":47638,"user":{"displayName":"Abdelkhalig Elfatih (Khal)","userId":"06815457852751117892"}}},"id":"y47ZhV2z3zbR","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# LOAD DATA\n","# ============================================================================\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Path configuration - adjust this to your folder structure\n","DATA_PATH = \"/content/drive/MyDrive/AIMS/NLP/dev_phase/subtask1\"\n","\n","try:\n","    train_df = pd.read_csv(f\"{DATA_PATH}/train/eng.csv\")\n","    test_df = pd.read_csv(f\"{DATA_PATH}/dev/eng.csv\")\n","    print(f\"\\nLoaded {len(train_df)} training rows from train/eng.csv\")\n","    print(f\"Loaded {len(test_df)} test rows from dev/eng.csv\")\n","    print(f\"Training columns: {train_df.columns.tolist()}\")\n","except FileNotFoundError as e:\n","    raise ValueError(f\"Could not find files in {DATA_PATH}. Please check the path.\\nError: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WCLMUBI3ImH","executionInfo":{"status":"ok","timestamp":1765893545569,"user_tz":-120,"elapsed":5646,"user":{"displayName":"Abdelkhalig Elfatih (Khal)","userId":"06815457852751117892"}},"outputId":"f2a6500f-68a9-4a1d-e835-a2cbfa82e01b"},"id":"9WCLMUBI3ImH","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Loaded 3222 training rows from train/eng.csv\n","Loaded 160 test rows from dev/eng.csv\n","Training columns: ['id', 'text', 'polarization']\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","MODEL_NAME = \"microsoft/deberta-v3-base\"\n","MAX_LENGTH = 256\n","BATCH_SIZE = 16\n","GRAD_ACCUMULATION = 1\n","LEARNING_RATE = 1e-5\n","EPOCHS = 8\n","WARMUP_RATIO = 0.1\n","\n","print(\"Configuration:\")\n","print(f\"  Model: {MODEL_NAME}\")\n","print(f\"  Max Length: {MAX_LENGTH}\")\n","print(f\"  Batch Size: {BATCH_SIZE}\")\n","print(f\"  Learning Rate: {LEARNING_RATE}\")\n","print(f\"  Epochs: {EPOCHS}\")\n","\n","# Check for GPU\n","if not torch.cuda.is_available():\n","    print(\"\\nWARNING: Running on CPU. Switch to GPU for faster training.\")\n","    print(\"Go to Runtime > Change runtime type > T4 GPU\")\n","else:\n","    print(f\"\\nGPU Detected: {torch.cuda.get_device_name(0)}\")\n","\n","# ============================================================================\n","# DATA PREPROCESSING\n","# ============================================================================\n","print(\"\\n--- Data Preprocessing ---\")\n","\n","# Clean training data\n","train_df['text'] = train_df['text'].fillna('').astype(str)\n","train_df['text'] = train_df['text'].str.strip()\n","train_df['polarization'] = train_df['polarization'].fillna(0).astype(int)\n","\n","# Remove empty texts\n","train_df = train_df[train_df['text'].str.len() > 0].reset_index(drop=True)\n","\n","# Ensure only valid labels (0 or 1)\n","train_df = train_df[train_df['polarization'].isin([0, 1])].reset_index(drop=True)\n","\n","print(f\"Cleaned training data shape: {train_df.shape}\")\n","print(f\"\\nLabel Distribution:\")\n","label_counts = train_df['polarization'].value_counts()\n","print(f\"  Non-Polarized (0): {label_counts[0]} ({label_counts[0]/len(train_df)*100:.2f}%)\")\n","print(f\"  Polarized (1): {label_counts[1]} ({label_counts[1]/len(train_df)*100:.2f}%)\")\n","\n","# Clean test data\n","test_df['text'] = test_df['text'].fillna('').astype(str)\n","test_df['text'] = test_df['text'].str.strip()\n","test_df = test_df[test_df['text'].str.len() > 0].reset_index(drop=True)\n","\n","print(f\"\\nCleaned test data shape: {test_df.shape}\")\n","\n","# ============================================================================\n","# CALCULATE CLASS WEIGHTS\n","# ============================================================================\n","total_samples = len(train_df)\n","pos_count = (train_df['polarization'] == 1).sum()\n","neg_count = (train_df['polarization'] == 0).sum()\n","\n","# Inverse frequency weights with smoothing\n","alpha = 0.5\n","weight_neg = (pos_count + alpha) / (neg_count + alpha)\n","weight_pos = (neg_count + alpha) / (pos_count + alpha)\n","\n","class_weights = torch.tensor([weight_neg, weight_pos], dtype=torch.float)\n","\n","print(\"\\n--- Class Weights (Balanced) ---\")\n","print(f\"  Non-Polarized (0): {weight_neg:.2f}x\")\n","print(f\"  Polarized (1): {weight_pos:.2f}x\")\n","\n","# ============================================================================\n","# TRAIN-VALIDATION SPLIT\n","# ============================================================================\n","train_texts = train_df['text'].values\n","train_labels = train_df['polarization'].values\n","\n","# Stratified 85-15 split\n","train_texts_split, val_texts_split, train_labels_split, val_labels_split = train_test_split(\n","    train_texts,\n","    train_labels,\n","    test_size=0.15,\n","    random_state=42,\n","    stratify=train_labels\n",")\n","\n","print(f\"\\n--- Data Split ---\")\n","print(f\"Training samples: {len(train_texts_split)}\")\n","print(f\"Validation samples: {len(val_texts_split)}\")\n","print(f\"Test samples: {len(test_df)}\")\n","\n","print(f\"\\nValidation Label Distribution:\")\n","val_unique, val_counts = np.unique(val_labels_split, return_counts=True)\n","for label, count in zip(val_unique, val_counts):\n","    print(f\"  Class {label}: {count} ({count/len(val_labels_split)*100:.2f}%)\")\n","\n","# ============================================================================\n","# DATASET CLASS\n","# ============================================================================\n","class PolarizationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len, is_test=False):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.is_test = is_test\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","\n","        inputs = self.tokenizer(\n","            text,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_len,\n","            return_tensors=\"pt\",\n","            add_special_tokens=True\n","        )\n","\n","        item = {k: v.squeeze() for k, v in inputs.items()}\n","\n","        if not self.is_test:\n","            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return item\n","\n","# Create tokenizer and datasets\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","train_dataset = PolarizationDataset(train_texts_split, train_labels_split, tokenizer, MAX_LENGTH)\n","val_dataset = PolarizationDataset(val_texts_split, val_labels_split, tokenizer, MAX_LENGTH)\n","\n","print(f\"\\nDatasets created:\")\n","print(f\"  Train dataset size: {len(train_dataset)}\")\n","print(f\"  Validation dataset size: {len(val_dataset)}\")\n","\n","# ============================================================================\n","# WEIGHTED TRAINER\n","# ============================================================================\n","class WeightedTrainer(Trainer):\n","    def __init__(self, *args, class_weights=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.class_weights = class_weights\n","\n","    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","        if self.class_weights is not None:\n","            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n","            loss = loss_fct(logits, labels)\n","        else:\n","            loss = outputs.loss\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","# ============================================================================\n","# METRICS COMPUTATION\n","# ============================================================================\n","def compute_metrics(p):\n","    predictions, labels = p\n","    preds = np.argmax(predictions, axis=1)\n","    macro_f1 = f1_score(labels, preds, average='macro', zero_division=0)\n","    return {'f1_macro': macro_f1}\n","\n","# ============================================================================\n","# MODEL INITIALIZATION\n","# ============================================================================\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=2,\n","    hidden_dropout_prob=0.2,\n","    attention_probs_dropout_prob=0.2\n",")\n","\n","print(\"\\nModel loaded successfully\")\n","\n","# ============================================================================\n","# TRAINING ARGUMENTS\n","# ============================================================================\n","training_args = TrainingArguments(\n","    output_dir=\"./results_subtask1\",\n","    num_train_epochs=EPOCHS,\n","    learning_rate=LEARNING_RATE,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE * 2,\n","    gradient_accumulation_steps=GRAD_ACCUMULATION,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1_macro\",\n","    greater_is_better=True,\n","    logging_steps=50,\n","    fp16=True,\n","    weight_decay=0.01,\n","    warmup_ratio=WARMUP_RATIO,\n","    report_to=\"none\",\n","    save_total_limit=1,\n","    gradient_checkpointing=False,\n","    dataloader_num_workers=2,\n","    lr_scheduler_type=\"cosine\",\n","    seed=42\n",")\n","\n","# ============================================================================\n","# TRAIN MODEL\n","# ============================================================================\n","trainer = WeightedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    class_weights=class_weights,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STARTING TRAINING\")\n","print(\"=\"*70 + \"\\n\")\n","\n","trainer.train()\n","\n","print(\"\\nTraining completed successfully\")\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","\n","# ============================================================================\n","# VISUALIZE & DOWNLOAD TRAINING RESULTS\n","# ============================================================================\n","\n","# 1. Extract logs from the trainer\n","history = trainer.state.log_history\n","df_log = pd.DataFrame(history)\n","\n","# 2. Separate training and validation data\n","# Training logs contain 'loss', validation logs contain 'eval_loss'\n","train_log = df_log[df_log['loss'].notna()][['epoch', 'loss']]\n","val_log = df_log[df_log['eval_loss'].notna()][['epoch', 'eval_loss', 'eval_f1_macro']]\n","\n","# 3. Create Plots\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n","\n","# Plot 1: Loss Curves\n","axes[0].plot(train_log['epoch'], train_log['loss'], label='Training Loss', alpha=0.7)\n","axes[0].plot(val_log['epoch'], val_log['eval_loss'], label='Validation Loss', marker='o', linewidth=2)\n","axes[0].set_title('Training & Validation Loss', fontsize=14)\n","axes[0].set_xlabel('Epochs')\n","axes[0].set_ylabel('Loss')\n","axes[0].legend()\n","axes[0].grid(True, linestyle='--', alpha=0.6)\n","\n","# Plot 2: F1 Score Curve\n","axes[1].plot(val_log['epoch'], val_log['eval_f1_macro'], label='Validation F1 Macro', color='green', marker='o', linewidth=2)\n","axes[1].set_title('Validation F1 Macro Score', fontsize=14)\n","axes[1].set_xlabel('Epochs')\n","axes[1].set_ylabel('F1 Macro Score')\n","axes[1].legend()\n","axes[1].grid(True, linestyle='--', alpha=0.6)\n","\n","plt.tight_layout()\n","\n","# 4. Save to PDF and Download\n","pdf_filename = \"training_metrics_plots.pdf\"\n","plt.savefig(pdf_filename, format='pdf', dpi=300)\n","print(f\"Plots saved to {pdf_filename}\")\n","\n","# Show the plot in the notebook\n","plt.show()\n","\n","# Trigger Download\n","try:\n","    files.download(pdf_filename)\n","except Exception as e:\n","    print(f\"Automatic download failed. Please download '{pdf_filename}' manually from the file explorer. Error: {e}\")\n","\n","# ============================================================================\n","# VALIDATION EVALUATION (0.5 threshold)\n","# ============================================================================\n","eval_stats = trainer.evaluate()\n","print(f\"\\n--- Validation Results (0.5 threshold) ---\")\n","print(f\"Macro F1: {eval_stats['eval_f1_macro']:.4f}\")\n","\n","# ============================================================================\n","# THRESHOLD OPTIMIZATION\n","# ============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"THRESHOLD OPTIMIZATION\")\n","print(\"=\"*70)\n","\n","raw_preds = trainer.predict(val_dataset)\n","logits = raw_preds.predictions\n","probs = expit(logits)  # Convert logits to probabilities using sigmoid\n","y_true = raw_preds.label_ids\n","\n","# Get probability of class 1 (Polarized)\n","positive_probs = probs[:, 1]\n","\n","# Grid search for best threshold\n","thresholds = np.arange(0.05, 0.95, 0.01)\n","best_f1 = 0\n","best_thresh = 0.5\n","\n","threshold_results = []\n","for t in thresholds:\n","    preds = (positive_probs >= t).astype(int)\n","    current_f1 = f1_score(y_true, preds, average='macro', zero_division=0)\n","    threshold_results.append({'threshold': t, 'f1_macro': current_f1})\n","    if current_f1 > best_f1:\n","        best_f1 = current_f1\n","        best_thresh = t\n","\n","print(f\"\\nBest Threshold: {best_thresh:.3f}\")\n","print(f\"Optimized Macro F1: {best_f1:.4f}\")\n","\n","# Plot threshold vs F1 score\n","threshold_df = pd.DataFrame(threshold_results)\n","plt.figure(figsize=(10, 6))\n","plt.plot(threshold_df['threshold'], threshold_df['f1_macro'], linewidth=2)\n","plt.axvline(x=best_thresh, color='r', linestyle='--', label=f'Best Threshold: {best_thresh:.3f}')\n","plt.xlabel('Decision Threshold', fontsize=12)\n","plt.ylabel('Macro F1 Score', fontsize=12)\n","plt.title('Threshold Optimization', fontsize=14, fontweight='bold')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","# Apply optimized threshold\n","final_preds = (positive_probs >= best_thresh).astype(int)\n","\n","# ============================================================================\n","# DETAILED VALIDATION ANALYSIS\n","# ============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"VALIDATION SET PERFORMANCE\")\n","print(\"=\"*70)\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_true, final_preds)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=['Non-Polarized', 'Polarized'],\n","            yticklabels=['Non-Polarized', 'Polarized'],\n","            cbar_kws={'label': 'Count'})\n","plt.ylabel('Actual', fontsize=12)\n","plt.xlabel('Predicted', fontsize=12)\n","plt.title(f'Confusion Matrix (Threshold: {best_thresh:.3f})', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","# Classification Report\n","print(\"\\nDetailed Classification Report:\")\n","print(classification_report(y_true, final_preds,\n","                          target_names=['Non-Polarized', 'Polarized'],\n","                          digits=4,\n","                          zero_division=0))\n","\n","# Per-class metrics\n","from sklearn.metrics import precision_score, recall_score\n","print(\"\\nPer-Class Metrics:\")\n","print(f\"  Non-Polarized Precision: {precision_score(y_true, final_preds, pos_label=0, zero_division=0):.4f}\")\n","print(f\"  Non-Polarized Recall:    {recall_score(y_true, final_preds, pos_label=0, zero_division=0):.4f}\")\n","print(f\"  Non-Polarized F1:        {f1_score(y_true, final_preds, pos_label=0, zero_division=0):.4f}\")\n","print(f\"  Polarized Precision:     {precision_score(y_true, final_preds, pos_label=1, zero_division=0):.4f}\")\n","print(f\"  Polarized Recall:        {recall_score(y_true, final_preds, pos_label=1, zero_division=0):.4f}\")\n","print(f\"  Polarized F1:            {f1_score(y_true, final_preds, pos_label=1, zero_division=0):.4f}\")\n","print(f\"  Macro F1:                {best_f1:.4f}\")\n","\n","# ============================================================================\n","# TEST SET PREDICTIONS\n","# ============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"GENERATING TEST SET PREDICTIONS\")\n","print(\"=\"*70)\n","\n","# Create test dataset\n","test_texts = test_df['text'].values\n","test_labels_dummy = np.zeros(len(test_texts))  # Dummy labels for test set\n","test_dataset = PolarizationDataset(test_texts, test_labels_dummy, tokenizer, MAX_LENGTH, is_test=True)\n","\n","# Get predictions\n","test_raw_preds = trainer.predict(test_dataset)\n","test_logits = test_raw_preds.predictions\n","test_probs = expit(test_logits)\n","test_positive_probs = test_probs[:, 1]\n","\n","# Apply optimized threshold\n","test_predictions = (test_positive_probs >= best_thresh).astype(int)\n","\n","# Create submission dataframe\n","submission = pd.DataFrame({\n","    'id': test_df['id'],\n","    'polarization': test_predictions\n","})\n","\n","print(f\"\\nTest predictions generated for {len(submission)} samples\")\n","print(f\"\\nPrediction Distribution:\")\n","pred_counts = submission['polarization'].value_counts()\n","print(f\"  Non-Polarized (0): {pred_counts[0]} ({pred_counts[0]/len(submission)*100:.2f}%)\")\n","print(f\"  Polarized (1): {pred_counts[1]} ({pred_counts[1]/len(submission)*100:.2f}%)\")\n","\n","# Display sample predictions\n","print(\"\\nSample Predictions (first 10 rows):\")\n","sample_display = submission.head(10).copy()\n","sample_display['text_preview'] = test_df['text'].head(10).str[:50] + '...'\n","print(sample_display.to_string(index=False))\n","\n","# Save submission file\n","submission_path = 'subtask1_submission.csv'\n","submission.to_csv(submission_path, index=False)\n","print(f\"\\nSubmission file saved: {submission_path}\")\n","\n","# Download file\n","try:\n","    from google.colab import files\n","    files.download(submission_path)\n","    print(\"Submission file downloaded to your computer\")\n","except:\n","    print(\"File saved but could not auto-download. Please download manually from files panel.\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"PROCESS COMPLETE\")\n","print(\"=\"*70)\n","print(\"\\nYour submission file is ready for competition upload!\")\n","print(f\"Final validation Macro F1: {best_f1:.4f}\")\n","print(f\"Optimized threshold: {best_thresh:.3f}\")"],"metadata":{"id":"W2PnyNSI28Z6","collapsed":true},"id":"W2PnyNSI28Z6","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}